import numpy as np
import pandas as pd

np.random.seed(7)

##softmax
def softmax(u):
  expu=np.exp(u)
  return expu/np.sum(expu)

##cross entropy loss func
def crossEntropy(p,q):
  return -np.vdot(p,np.log(q))

##cost evaluation
def eval_L(X,Y,beta):
  N=X.shape[0]
  L=0.0
  for i in range(N):
    XiHat=X[i]
    Yi=Y[i]
    qi=softmax(beta @ XiHat)

    L+=crossEntropy(Yi,qi)

  return L

##logisticregression function(stochastic)
def logReg_SGD(X,Y,lr):
  numEpochs=5 ##epochs defined
  N,d = X.shape   ##N=n_samples,d=n_features
  X=np.insert(X,0,1,axis=1)
  k=Y.shape[1]
  beta=np.zeros((k,d+1))
  Lvals=[]
  for ep in range(numEpochs):

    L=eval_L(X,Y,beta)
    Lvals.append(L)

    print("Epoch is: "+str(ep)+" Cost is:"+str(L))

    prm=np.random.permutation(N)
    for i in prm:
      XiHat=X[i]
      Yi=Y[i]

      qi=softmax(beta @ XiHat)
      grad_Li=np.outer(qi-Yi,XiHat)

      beta-=lr*grad_Li

    return beta,Lvals

##predictor
def predict(X,beta):
  X=np.insert(X,0,1,axis=1)
  N=X.shape[0]

  predictions=[]
  for i in range (N):
    XiHat=X[i]
    qi=softmax(beta @ XiHat)

    p=np.argmax(qi)

    predictions.append(p)

  return predictions

df_train = pd.read_csv('sample_data/mnist_train_small.csv')
df_test = pd.read_csv('sample_data/mnist_test.csv')

X_train=df_train.drop(df_train.columns[0], axis=1)
X_test=df_test.drop(df_test.columns[0], axis=1)

Y_train=df_train.iloc[:,0]
Y_test=df_test.iloc[:,0]

X_train = X_train/255.0
X_test = X_test/255.0

N_train, numRows, numCols = X_train.shape
X_train = np.reshape(X_train, (N_train, numRows * numCols))

Y_train = pd.get_dummies(Y_train).values

alpha = 0.01
beta, Lvals= logReg_SGD(X_train, Y_train, alpha)

plt.semilogy(Lvals)

N_test = X_test.shape[0]
X_test = np.reshape(X_test,(N_train,numRows*numCols))
predictions = predict(X_test, beta)

numCorrect = 0
for i in range(N_Test):

    if predictions[i] == Y_test[i]:
      numCorrect += 1

accuracy = numCorrect/N_test
print('accuracy: '+ str(accuracy))
